{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\SPARK_HOME'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local') #Основным механизмом настройки в Spark \n",
    "                                                                    #является класс SparkConf.\n",
    "#conf = new SparkConf()\n",
    "#conf.set(\"spark.app.name\", \"Му Spark Арр\")\n",
    "#conf.set(\"spark.master\", \"local[4]\")\n",
    "#conf.set(\"spark.ui.port\", \"36000\") # Переопределить порт по умолчанию\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf) # Создать SparkContext с данной конфигурацией\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.144:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>appName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=appName>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Программирование операций с RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"C:\\spark\\SPARK_HOME\\README.md\") #Создание набора RDD путем считывания файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "# Apache Spark\n"
     ]
    }
   ],
   "source": [
    "print lines.count() #вызов действия (не создает новый RDD)\n",
    "print lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:\\spark\\SPARK_HOME\\README.md MapPartitionsRDD[1] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonLines = lines.filter ( lambda line: \"Python\" in line) # вызов преобразования (создает новое RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PipelinedRDD.persist of PythonRDD[8] at RDD at PythonRDD.scala:53>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLines.persist # сохранение RDD pythonLines в памяти"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"pandas\", \"i like pandas\"]) #создание RDD способ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"C:\\spark\\SPARK_HOME\\README.md\") #Создание набора RDD путем считывания файла (cпособ 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*RDD_out = in_RDD.filter (lambda x: \"word\" in x) #фильтрация\n",
    "\n",
    "*RDD_out = in_RDD.map(lambda x: x*x) # функция map принимает в качестве аргумента другую функцию и применяет ее к каждому \n",
    "                                      # элементу RDD \n",
    "                                       \n",
    "*RDD_out = in_RDD.flatMap(lambda x: x.split(\" \")) # функция flatMap принимает в качестве аргумента другую функцию и применяет \n",
    "                                                  #  ее к каждому элементу RDD. Но вместо единственного элемента наша функция \n",
    "                                                  #  должна вернуть итератор для обхода возвращаемых значений новых элементов.\n",
    "                                                  #  Однако вместо набора RDD итераторов flatMap() возвращает набор RDD с \n",
    "                                                 #  элементами, получаемыми с применением всех итераторов.                      \n",
    "                                                    \n",
    "*RDD_final = RDD_1.union(RDD_2) #объединение двух RDD\n",
    "\n",
    "*RDD.distinct() #возвращает RDD без повторов элементов (только уникальные элементы)\n",
    "\n",
    "*RDD1.intersection(RDD2) # пересечение элементов двух RDD\n",
    "\n",
    "*RDD1.substract(RDD2) # вычитание из множества элементов RDD1 множества элементов RDD2\n",
    "\n",
    "*RDD1.cartesian(RDD2) #декартово произведение двух RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*RDD.count() #число элементов\n",
    "\n",
    "*for line in RDD.take(n): #извлечение n первых элементов RDD\n",
    "    print line\n",
    "    \n",
    "*RDD.collect() # извлечение всех элеметов rdd в память\n",
    "\n",
    "*RDD.reduce(lambda x,y: x+y) #принимает функцию, оперирующую двумя элементами данного RDD в качестве аргумента и возвращает \n",
    "                            #элемент того же типа\n",
    "                              \n",
    "*RDD.fold(0)(lambda x,y: x+y) #то же самое что и reduce, но с указанием нулевого значения\n",
    "\n",
    "*RDD.aggregate((0,0), #начальное нулевое значение типа, который следует вернуть \n",
    "               (lambda acc,value: (acc[0]+value, acc[1]+1)), # функция для объединения двух элементов с накопителем\n",
    "               (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) #функция для объединения результатов двух узлов\n",
    "              )\n",
    "              \n",
    "*RDD.top() #возвращает верхние элементы RDD\n",
    "\n",
    "*RDD.takeSample(withReplacement, num, seed) #возвращает случайные элементы коллекции, num - число элементов, \n",
    "                                            # withReplacement(true or false) - c заменой или без\n",
    "                                             \n",
    "*RDD.foreach(func) # выполнить действие над всеми элементами в наборе RDD, но без возврата результата в программу-драйвер\n",
    "\n",
    "*RDD.countByValue () # возвращает отображение каждого уникального значения на его число (определяющее, сколько раз это значение \n",
    "                      # встречается в наборе)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Передача функций в SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = RDD.filter(lambda s: \"error\" in s) # c помощью lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsError(s): # c помощью определения пользовательской функции\n",
    "    return \"error\" in s\n",
    "word = rdd.filter(containsError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с парами ключ/значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.mapValues(func) # применить func к каждому значению без изменения ключа\n",
    "RDD.keys() # получить RDD из ключей\n",
    "RDD.values() # получить RDD из значений\n",
    "RDD.flatMapValues(func) # Применить функцию, возвращающую итератор для каждого значения в наборе пар и создающую \n",
    "                        # для каждого возвращаемого элемента пару ключ/значение со старым ключом. \n",
    "#АГРЕГИРОВАНИЕ\n",
    "RDD.combineByKey(createCombiner, mergeValue, mergeCombiners,partitioner) #Объединить значения с одинаковыми ключами и получить результат другого типа\n",
    "#createCombiner(value) - пользовательская функция, запускающаяся при встрече с новым ключом (создает начальное значение для счетчика)\n",
    "#mergeValue(acc,value) - пользовательская функция (с текущим значением счетчика и новым значением), запускающаяся при встрече с ключом, \n",
    "            #который уже встречался\n",
    "#mergeCombiners(acc1,acc2) - пользовательская функци, запускающаяся при объединении результатов обработки разных разделов если обнаружится\n",
    "            #несколько значений аккумулятора для одного и того же ключа\n",
    "#partitioner - объект управления распределением \n",
    "RDD.reduceByKey(func) # объединить значения с одинаковыми ключами (применить func к значениям с одинаковыми ключами)\n",
    "\n",
    "#Настройка уровня параллелизма\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) #второй параметр функции определяет уровень параллелизма\n",
    "RDD.repartion() #перераспределение данных\n",
    "RDD.coalesce() #оптимизированная версия repartion\n",
    "RDD.partitions.size() #размер RDD\n",
    "RDD.partitionBy(n, func) #разбиение RDD на n разделов, определяет, сколько параллельных заданий будет создаваться при выполнении\n",
    "                    #операций над этим набором RDD, func - функция хэширования\n",
    "\n",
    "#Группировка данных\n",
    "RDD.groupByKey() # сгруппировать значения с одинаковыми ключами (на выходе каждому ключу соответствует кортеж значений \n",
    "                     # имеющих этот ключ)\n",
    "RDD.groupBy(func) #применяет func к каждому элементу RDD, полученный результат использует как ключ\n",
    "\n",
    "RDD1.cogroup(RDD2) #операция выполняется над двумя наборами RDD с ключами одного типа К и значениями типа V и W, она \n",
    "                    #возвращает набор типа RDD [ (К, ( Iterable [V], Iterable [W]))]\n",
    "\n",
    "#Соединения\n",
    "RDD1.join(RDD2) #INNER JOIN\n",
    "RDD1.leftOuterJoin(RDD2) #внешнее левостороннее соединение\n",
    "RDD1.rightOuterJoin(RDD2) #внешнее правостороннее соединение\n",
    "\n",
    "#Сортировка\n",
    "RDD.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x)) # отсортировать по значениям ключей\n",
    "                                                                              #ascending - по возрастанию или убыванию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'', 72), (u'project.', 1), (u'help', 1), (u'when', 1), (u'Hadoop', 3)]\n"
     ]
    }
   ],
   "source": [
    "#Пример. Подсчет слов\n",
    "rdd = sc.textFile(\"C:\\spark\\SPARK_HOME\\README.md\")\n",
    "words = rdd.flatMap(lambda x: x.split(\" \")) #создание RDD, где каждый элемент отдельное слово\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y) \n",
    "print result.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (3, 2)), (1, (7, 2))]\n",
      "[(0, 1.5), (1, 3.5)]\n"
     ]
    }
   ],
   "source": [
    "# Пример 4. 12. Вычисление среднего значения для каждого ключа с помощью combineByKey() в Python\n",
    "nums = sc.parallelize([(0,1), (0,2), (1,3), (1,4)])\n",
    "sumCount = nums.combineByKey(\n",
    "(lambda x: (x,1)), #x - соответсв значение в паре ключ-значение\n",
    "(lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "(lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "print(sumCount.collect())\n",
    "print(sumCount.map(lambda (key,(summa, count)): (key, summa/float(count))).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Действия над наборами RDD c парами ключ-значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.countByKey() #Подсчет числа элементов  для каждого ключа\n",
    "rdd.collectAsMap() # Извлечение данных в виде  словаря\n",
    "rdd.lookup(key) # Извлечение всех значений, связанных с указанным ключом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАГРУЗКА И СОХРАНЕНИЕ ДАННЫХ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Текстовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = sc.textFile(\"C:\\spark\\SPARK_HOME\\README.md\") # загрузка текстового файла, каждое слово - отдельный элемент RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'file:/C:/spark/SPARK_HOME/1.txt',\n",
       " u'file:/C:/spark/SPARK_HOME/LICENSE',\n",
       " u'file:/C:/spark/SPARK_HOME/NOTICE',\n",
       " u'file:/C:/spark/SPARK_HOME/README.md',\n",
       " u'file:/C:/spark/SPARK_HOME/RELEASE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = sc.wholeTextFiles(\"C:\\spark\\SPARK_HOME\\\\\") #загрузка всех текстовых файлов, содержащихся в директории SPARK_HOME, \n",
    "                                                    #в формате ключ-значение, где ключ - имена файлов\n",
    "input.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.saveAsTextFile(outputFile) #сохранение RDD result в текстовый файл outputFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input = sc.textFile(\"C:\\spark\\SPARK_HOME\\name_file.json\") #загрузка json файла как текстового (одна строка - одна JSON запись)\n",
    "data = input.map(lambda x: json.loads(x)) #парсер json файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.map(lambda x: json.dumps(x)).saveAsTextFile(outputFile) #сохранение данных в json формате"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import StringIO\n",
    "#загрузка одного файла\n",
    "def loadRecord(line):\n",
    "    \"\"\"Парсинг строки CSV\"\"\"\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, fieldnames=[\"name\", \"favouriteAnimal\"])\n",
    "    return reader.next()\n",
    "input = sc.textFile(\"C:\\spark\\SPARK_HOME\\name_file.csv\").map(loadRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка всех файлов целиком\n",
    "def loadRecords(fileNameContents):\n",
    "    \"\"\"Загружает все записи из заданного файла\"\"\"\n",
    "    input = StringIO.StringIO(fileNameContents[1])\n",
    "    reader = csv.DictReader(input, fieldnames=[\"name\", \"favoriteAnimal\"])\n",
    "    return reader\n",
    "fullFileData = sc.wholeTextFiles(\"C:\\spark\\SPARK_HOME\\\\\").flatMap(loadRecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запись в файл\n",
    "def writeRecords(records): \n",
    "    \"\"\"Записывает в файл строки CSV\"\"\"\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames= [\"name\", \"favoriteAnimal\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SequenceFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceFiles - популярный формат файлов, используемый в Hadoop, состоящих из пар ключ/значение.\n",
    "\n",
    "sequenceFile(path, keyClass, valueClass, minPartitions) - функция загрузки\n",
    "\n",
    "path - путь к файлу\n",
    "\n",
    "keyClass - формат ключей\n",
    "\n",
    "valueClass - формат значений\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.sequenceFile(inFile, \"org.apache.hadoop.io.Text\", \"org.apache.hadoop.io.IntWritаblе\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(list((\"Panda\", 3), (\"Кау\", 6), (\"Snail\", 2)))\n",
    "data.saveAsSequenceFile(outputFile)  #сохранение в формате sequenceFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объектные файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = sc.pickleFile(\"C:\\spark\\SPARK_HOME\\file_name\")\n",
    "input.saveAsPickleFile(outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РАЗДЕЛЯЕМЫЕ ПЕРЕМЕННЫЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Аккумуляторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank lines: 40\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile('C:\\spark\\SPARK_HOME\\README.md')\n",
    "\n",
    "blankLines = sc.accumulator(0) # Создать Accumulator[Int], инициализированный нулем\n",
    "\n",
    "def extractCallSigns(line):\n",
    "    global blankLines # Получить доступ к глобальной переменной\n",
    "    if (line == \"\"):\n",
    "        blankLines += 1\n",
    "    return line.split(\" \")\n",
    "\n",
    "callSigns = file.flatMap(extractCallSigns)\n",
    "callSigns.saveAsTextFile(\"C:\\spark\\SPARK_HOME\\\\\" + \"callsigns\")\n",
    "print \"Blank lines: %d\" % blankLines.value #доступ к значению аккумулятора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Широковещательные переменные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Широковещательные переменные (bгoadcast vmiaЬles) - позволяют программам эффективно передавать большие значения, доступные только для чтения, всем рабочим узлам для использования в операциях Spark.\n",
    "Переменная будет отправлена каждому узлу лишь один раз и должна использоваться только для чтения (изменения в переменной не будут передаваться другим узлам)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример 6.7: Поиск страны с помощью широковещательной переменной в Python\n",
    "# Поиск страны по позывному в наборе RDD coпtactCouпts. С этой целью загружается массив кодов стран с соответствующими\n",
    "# префиксами позывных.\n",
    "signPrefixes = sc.broadcast(loadCallSignTable()) #cоздание широковещательной переменной\n",
    "\n",
    "def processSignCount (sign count, signPrefixes) :\n",
    "    country = lookupCountry(sign_count[0], signPrefixes.value) #метод value предоставляет доступ к значению широковещательной переменной\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "countryContactCounts = (contactCounts.map(processSignCount).reduceByKey((lambda x, y: x+y)))\n",
    "countryContactCounts.saveAsTextFile(outputDir + \"/countries.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Взаимодейсвтие с внешними программами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наборы RDD в Spark имеют метод pipe (), благодаря которому можно писать задания для Spark на любом языке, при условии что\n",
    "они будут записывать данные в стандартные потоки ввода/вывода Unix и читать их оттуда."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Числовые операuии над наборами RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все описательные статистики вычисляются за один проход по данным и возвращаются методом stats () в виде объекта StatsCounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD.count() - Число элементов в наборе\n",
    "\n",
    "RDD.mean() - Среднее значение по элементам\n",
    "\n",
    "RDD.sum() - Общую сумму элементов\n",
    "\n",
    "RDD.max() - Максимальное значение\n",
    "\n",
    "RDD.min() - Минимальное значение\n",
    "\n",
    "RDD.variance() - Дисперсию элементов\n",
    "\n",
    "RDD.sampleVariance() - Дисперсию для выборки элементов\n",
    "\n",
    "RDD.stdev() - Стандартное отклонение\n",
    "\n",
    "RDD.sampleStdev() - Стандартное отклонение для выборки элементов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats = RDD.stats() - Вычисляет все статистки сразу же\n",
    "\n",
    "mean = stats.mean() - доступ к статистикам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выполнение в кластере\n",
    "\n",
    "Драйвер экспортирует информацию о выполняемом приложении Spark через веб-интерфейс: http://localhost:4040 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск приложения в кластере с помощью Spark:\n",
    "\n",
    "1. Пользователь вызывает сценарий spark-submit, чтобы запустить приложение.\n",
    "\n",
    "2. spark-submit запускает· программу-драйвер и вызывает метод main (), указанный пользователем.\n",
    "\n",
    "3. Программа-драйвер связывается с диспетчером кластера и запрашивает у него ресурсы для запуска исполнителей.\n",
    "\n",
    "4. Диспетчер кластера запускает исполнителей от имени программы- драйвера.\n",
    "\n",
    "5. Процесс драйвера выполняет инструкции в пользовательском приложении. Опираясь на действия и преобразования наборов RDD в программе, драйвер посылает задания исполнителям.\n",
    "\n",
    "6. Исполнители выполняют задания, получают и сохраняют результаты.\n",
    "\n",
    "7. Если драйвер выполняет выход из метода main () или вызывает SparkContext. stop (), Spark останавливает исполнителей и освобождает ресурсы, возвращая их диспетчеру кластера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 72\n",
      "project.: 1\n",
      "help: 1\n",
      "when: 1\n",
      "Hadoop: 3\n",
      "MLlib: 1\n",
      "\"local\": 1\n",
      "./dev/run-tests: 1\n",
      "including: 4\n",
      "computation: 1\n",
      "file: 1\n",
      "high-level: 1\n",
      "find: 1\n",
      "web: 1\n",
      "Shell: 2\n",
      "cluster: 2\n",
      "Kubernetes: 1\n",
      "also: 5\n",
      "using:: 1\n",
      "Big: 1\n",
      "guidance: 2\n",
      "run:: 1\n",
      "Scala,: 1\n",
      "Running: 1\n",
      "should: 2\n",
      "environment: 1\n",
      "to: 17\n",
      "only: 1\n",
      "module,: 1\n",
      "given.: 1\n",
      "rich: 1\n",
      "directory.: 1\n",
      "Apache: 1\n",
      "Interactive: 2\n",
      "sc.parallelize(range(1000)).count(): 1\n",
      "Building: 1\n",
      "do: 2\n",
      "guide,: 1\n",
      "return: 2\n",
      "which: 2\n",
      "Programs: 1\n",
      "Many: 1\n",
      "Enabling: 1\n",
      "built,: 1\n",
      "YARN,: 1\n",
      "R,: 1\n",
      "using: 3\n",
      "Example: 1\n",
      "scala>: 1\n",
      "Once: 1\n",
      "-DskipTests: 1\n",
      "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).: 1\n",
      "and: 10\n",
      "Because: 1\n",
      "cluster.: 1\n",
      "name: 1\n",
      "Testing: 1\n",
      "Streaming: 1\n",
      "./bin/pyspark: 1\n",
      "SQL: 2\n",
      "through: 1\n",
      "GraphX: 1\n",
      "them,: 1\n",
      "guide](http://spark.apache.org/contributing.html): 1\n",
      "[run: 1\n",
      "resource-managers/kubernetes/integration-tests/README.md: 1\n",
      "development: 1\n",
      "abbreviated: 1\n",
      "set: 2\n",
      "For: 3\n",
      "Scala: 2\n",
      "##: 9\n",
      "the: 23\n",
      "thread,: 1\n",
      "library: 1\n",
      "see: 3\n",
      "individual: 1\n",
      "examples: 2\n",
      "MASTER: 1\n",
      "runs.: 1\n",
      "[Apache: 1\n",
      "Pi: 1\n",
      "instructions.: 1\n",
      "More: 1\n",
      "Python,: 2\n",
      "#: 1\n",
      "processing,: 1\n",
      "for: 12\n",
      "review: 1\n",
      "integration: 1\n",
      "test,: 1\n",
      "contributing: 1\n",
      "Developer: 1\n",
      "version: 1\n",
      "provides: 1\n",
      "print: 1\n",
      "get: 1\n",
      "Configuration: 1\n",
      "supports: 2\n",
      "command,: 2\n",
      "[params]`.: 1\n",
      "refer: 2\n",
      "available: 1\n",
      "core: 1\n",
      "Guide](http://spark.apache.org/docs/latest/configuration.html): 1\n",
      "run: 7\n",
      "./bin/run-example: 2\n",
      "Versions: 1\n",
      "This: 2\n",
      "Hadoop,: 2\n",
      "Documentation: 1\n",
      "use: 3\n",
      "downloaded: 1\n",
      "distributions.: 1\n",
      "Spark.: 1\n",
      "latest: 1\n",
      "example:: 1\n",
      "`examples`: 2\n",
      "package.: 1\n",
      "Maven](http://maven.apache.org/).: 1\n",
      "[\"Building: 1\n",
      "package: 1\n",
      "of: 5\n",
      "changed: 1\n",
      "programming: 1\n",
      "optimized: 1\n",
      "against: 1\n",
      "site,: 1\n",
      "graph: 1\n",
      "or: 3\n",
      "comes: 1\n",
      "first: 1\n",
      "info: 1\n",
      "contains: 1\n",
      "can: 6\n",
      "overview: 1\n",
      "package.): 1\n",
      "There: 1\n",
      "Please: 4\n",
      "one: 2\n",
      "Contributing: 1\n",
      "(You: 1\n",
      "Online: 1\n",
      "tools: 1\n",
      "your: 1\n",
      "page](http://spark.apache.org/documentation.html).: 1\n",
      "threads.: 1\n",
      "Tests: 1\n",
      "fast: 1\n",
      "from: 1\n",
      "[project: 1\n",
      "APIs: 1\n",
      "\"yarn\": 1\n",
      "SparkPi: 2\n",
      "locally: 2\n",
      "system: 1\n",
      "submit: 1\n",
      "systems.: 1\n",
      "start: 1\n",
      "IDE,: 1\n",
      "params: 1\n",
      "build/mvn: 1\n",
      "way: 1\n",
      "basic: 1\n",
      "README: 1\n",
      "<http://spark.apache.org/>: 1\n",
      "It: 2\n",
      "engine: 1\n",
      "project: 1\n",
      "configure: 1\n",
      "on: 7\n",
      "started: 1\n",
      "Note: 1\n",
      "N: 1\n",
      "usage: 1\n",
      "versions: 1\n",
      "DataFrames,: 1\n",
      "particular: 2\n",
      "instance:: 1\n",
      "./bin/spark-shell: 1\n",
      "be: 2\n",
      "general: 3\n",
      "with: 3\n",
      "easiest: 1\n",
      "protocols: 1\n",
      "must: 1\n",
      "And: 1\n",
      "YARN\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn): 1\n",
      "developing: 1\n",
      "this: 1\n",
      "setup: 1\n",
      "shell:: 2\n",
      "will: 1\n",
      "Version: 1\n",
      "`./bin/run-example: 1\n",
      "following: 2\n",
      "Hadoop-supported: 1\n",
      "distribution: 1\n",
      "example: 3\n",
      "are: 1\n",
      "detailed: 2\n",
      "Data.: 1\n",
      "mesos://: 1\n",
      "stream: 1\n",
      "computing: 1\n",
      "URL,: 1\n",
      "is: 7\n",
      "in: 5\n",
      "higher-level: 1\n",
      "tests: 2\n",
      "1000:: 2\n",
      "an: 4\n",
      "sample: 1\n",
      "To: 2\n",
      "tests](http://spark.apache.org/developer-tools.html#individual-tests).: 1\n",
      "tips,: 1\n",
      "at: 2\n",
      "have: 1\n",
      "1000).count(): 1\n",
      "[\"Specifying: 1\n",
      "[building: 1\n",
      "You: 3\n",
      "if: 4\n",
      "Spark: 15\n",
      "information: 1\n",
      "different: 1\n",
      "Tools\"](http://spark.apache.org/developer-tools.html).: 1\n",
      "MASTER=spark://host:7077: 1\n",
      "no: 1\n",
      "not: 1\n",
      "Java,: 1\n",
      "that: 2\n",
      "storage: 1\n",
      "documentation,: 1\n",
      "same: 1\n",
      "machine: 1\n",
      "how: 3\n",
      "need: 1\n",
      "other: 1\n",
      "analysis.: 1\n",
      "build: 3\n",
      "prefer: 1\n",
      "online: 1\n",
      "you: 4\n",
      "several: 1\n",
      "[Contribution: 1\n",
      "A: 1\n",
      "About: 1\n",
      "HDFS: 1\n",
      "[Configuration: 1\n",
      "sc.parallelize(1: 1\n",
      "locally.: 1\n",
      "Hive: 2\n",
      "[\"Useful: 1\n",
      "running: 1\n",
      "uses: 1\n",
      "a: 9\n",
      ">>>: 1\n",
      "variable: 1\n",
      "The: 1\n",
      "data: 1\n",
      "class: 2\n",
      "built: 1\n",
      "building: 2\n",
      "Try: 1\n",
      "Python: 2\n",
      "Thriftserver: 1\n",
      "processing.: 1\n",
      "programs: 2\n",
      "requires: 1\n",
      "documentation: 3\n",
      "pre-built: 1\n",
      "Alternatively,: 1\n",
      "programs,: 1\n",
      "\"local[N]\": 1\n",
      "Spark](#building-spark).: 1\n",
      "clean: 1\n",
      "<class>: 1\n",
      "spark://: 1\n",
      "learning,: 1\n",
      "its: 1\n",
      "talk: 1\n",
      "graphs: 1\n"
     ]
    }
   ],
   "source": [
    "%run -i C:\\spark\\SPARK_HOME\\examples\\src\\main\\python\\wordcount.py C:\\spark\\SPARK_HOME\\README.md #запуск скрипта python (.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK-SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запуск с командной строки ANACONDA через сценарий spark-submit (локально)\n",
    "spark-submit C:\\spark\\SPARK_HOME\\examples\\src\\main\\python\\wordcount.py C:\\spark\\SPARK_HOME\\README.md\n",
    "#запуск с командной строки ANACONDA как скрипт python\n",
    "python C:\\spark\\SPARK_HOME\\examples\\src\\main\\python\\wordcount.py C:\\spark\\SPARK_HOME\\README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример 7 .З •:• Общий синтаксис командной строки spark-submit\n",
    "\n",
    "bin/spark-submit [options] <арр jar I python file> [арр options]\n",
    "\n",
    "1) [options] - список флагов для spark-subrnit; получить полный список флагов можно, выполнив команду spark-subrnit --help;\n",
    "\n",
    "2) <арр jar I python file> - имяJАR-файла или сценария на Python, содержащего точку входа в приложение;\n",
    "\n",
    "3) [арр options] - параметры для передачи приложению; если метод main () программы предусматривает анализ параметров командной строки, он увидит только параметры [ арр options], флаги, предназначенные для spark-submit, будут ему недоступны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Таблица 7. 1. Возможные значения для флага --master сценария spark-submit\n",
    "\n",
    "spark:/host:port  - Адрес кластера Spark Standalone с указанным портом. По умолчанию диспетчеры кластеров \n",
    "                    Spark Standalone используют порт 7077\n",
    "            \n",
    "mesos://host:port - Адрес кластера Mesos с указанным портом. По умолчанию диспетчеры кластеров Mesos используют порт 5050\n",
    "        \n",
    "yarn  -  Адрес кластера YARN. Перед запуском YARN необходимо определить переменную окружения HADOOP CONF DIR, \n",
    "         в которой указать путь к каталогу с настройками Hadoop, описывающими параметры кластера\n",
    "    \n",
    "local -  Запуск в локальном режиме на одном ядре\n",
    "\n",
    "local[N] -  Запуск в локальном режиме на N ядрах\n",
    "\n",
    "local[*] -  Запуск в локальном режиме на всех ядрах, имеющихся на машине"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Диспетчер кластера Spark Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустить кластер вручную можно с помощью сценария spark-class в подкаталоге bin/ каталога установки Spark. \n",
    "\n",
    "На ведущей машине выполните команду:\n",
    "bin/spark-class org.apache.spark.deploy.master.Master\n",
    "\n",
    "Затем на ведомых машинах:\n",
    "bin/spark-class org.apache.spark.deploy.worker.Worker spark://masternode:7077\n",
    "(где masternode - сетевое имя ведущей машины)\n",
    "\n",
    "Точно таким же способом можно запустить на кластере spark-shell или pyspark, передав флаг --master:\n",
    "\n",
    "spark-shell --rnaster spark://masternode:7077\n",
    "\n",
    "pyspark --rnaster spark://masternode:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK SQL\n",
    "\n",
    "Spark SQL определяет специальный тип RDD с именем SchernaRDD. Класс SchernaRDD представляет набор RDD объектов Row, каждый из которых представляет отдельную запись."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы задействовать Spark SQL, необходимо создать объект HiveContext (или SQLContext, когда поддержка Hive недоступна), опираясь на объект SparkContext. Этот объект контекста предоставляет дополнительные функции для получения и обработки данных. С помощью HiveContext можно создавать наборы данных SchemaRDD (DataFrame), представляющие структурированные данные, и оперировать ими с применением запросов SQL или обычных операций над RDD, таких как map ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортировать Spark SQL\n",
    "from pyspark.sql import HiveContext, Row\n",
    "#from pyspark.sql import SQLContext, Row # Или если не должно быть зависимостей от Hive\n",
    "#sqlCtx = SQLContext(sc) #Создание экземпляра контекста SQL в Python\n",
    "hiveCtx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=u'Adventures With Coffee, Code, and Writing.', retweetCount=0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример 9.11. Простой запрос SQL\n",
    "inputFile = \"testweet.json\"\n",
    "#inputFile = \"C:\\spark\\SPARK_HOME\\README.md\"\n",
    "input = hiveCtx.read.json(inputFile)\n",
    "input.registerTempTable(\"tweets\") # Зарегистрировать схему RDD\n",
    "topTweets = hiveCtx.sql(\"\"\"SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10\"\"\") # Выбрать сообщения по retweetCouпt\n",
    "topTweets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|                text|retweetCount|\n",
      "+--------------------+------------+\n",
      "|Adventures With C...|           0|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topTweets.show() #выводтаблицы в формате sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наборы SchemaRDD являются самыми обычными наборами данных, поэтому к ним могут применяться любые преобразования, такие как\n",
    "map () и filter (). Но, кроме этого, они обладают дополнительными функциональными возможностями. Самая важная из них - возможность зарегистрировать любой набор SchemaRDD как временную таблицу и выполнять запросы к ней с вызовом метода HiveContext.sql () или SQLContext.sql ()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Операции над объектами Row\n",
    "\n",
    "Объекты Row представляют записи внутри наборов SchemaRDD и фактически являются массивами полей фиксированной длины.\n",
    "В Python отсутствует явная типизация. Программа может просто обращаться к i-му элементу как row [i]. Кроме того, объекты Row в Python поддерживают доступ к полям по именам, в форме row.column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Adventures With Coffee, Code, and Writing.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topTweetText = topTweets.rdd.map(lambda row: row.text) #обращение к текстовому полю путем перевода dataframe в rdd\n",
    "topTweetText.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _1|\n",
      "+--------------------+\n",
      "|Adventures With C...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_topTweet = topTweetText.map(lambda x: (x, )).toDF() #создание df из rdd, способ 1\n",
    "df_topTweet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|favouriteBeverage|  name|\n",
      "+-----------------+------+\n",
      "|           coffee|holden|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "happyPeopleRDD = sc.parallelize([Row(name=\"holden\", favouriteBeverage=\"coffee\")]) #создание df из rdd способ 2\n",
    "happyPeopleSchemaRDD = hiveCtx.createDataFrame(happyPeopleRDD)\n",
    "happyPeopleSchemaRDD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции, определяемые польэователем в SPARK_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|strLenPython(text)|\n",
      "+------------------+\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveCtx.registerFunction(\"strLenPython\", lambda x: len (x), pyspark.sql.types.IntegerType()) #определение пользовательской функии\n",
    "lengthSchemaRDD = hiveCtx.sql(\"SELECT strLenPython('text') FROM tweets LIMIT 10\")\n",
    "lengthSchemaRDD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение с MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# пример. Спам/не спам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF #частота слов\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = sc.textFile(\"spam.txt\")\n",
    "normal = sc.textFile(\"ham.txt\")\n",
    "# Создать экземпляр HashingTF для отображения текста электронных писем в векторы с 10 ООО признаков.\n",
    "tf = HashingTF(numFeatures = 10000)\n",
    "# Разбить каждое электронное пистьмо на слова и каждое слово отобразить в один признак.\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "normalFeatures = normal.map(lambda email: tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[6] at union at <unknown>:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать наборы данных LabeledPoint для примеров, дающих положительную реакцию (спам) и отрицательную (обычные письма).\n",
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))\n",
    "trainingData = positiveExamples.union(negativeExamples)\n",
    "trainingData.cache() # Кэшировать, потому что алгоритм Logistic Regression является циклическим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for positive test example: 1\n",
      "Prediction for negative test example: 0\n"
     ]
    }
   ],
   "source": [
    "# Выполнить логистическую регрессию методом SGD.\n",
    "model = LogisticRegressionWithSGD.train(trainingData)\n",
    "#Проверить положительный экземпляр (спам) и отрицательный (неспам).\n",
    "#Сначала применить то же преобразование HashingTF, чтобы получить векторы признаков, затем применить модель.\n",
    "posTest = tf.transform( \"О М G GET cheap stuff Ьу sending money to ... \".split (\" \"))\n",
    "negTest = tf.transform( \"Hi Dad, I started studying Spark the other ... \".split(\" \"))\n",
    "print \"Prediction for positive test example: %g\" %model.predict(posTest)\n",
    "print \"Prediction for negative test example: %g\" %model.predict(negTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Типы данных в MLlib\n",
    "\n",
    "1) Vector - вектор в математическом смысле. MLlib поддерживает обе разновидности векторов: плотные, хранящие все элементы, и разреженные, хранящие только ненулевые значения для экономии памяти. Векторы могут конструироваться с помощью класса mllib.linalg.Vector\n",
    "\n",
    "2) LabeledPoint - маркированная точка в пространстве данных для использования в алгоритмах обучения, таких как классификация и регрессия. Включает вектор признаков и маркер (являющийся вещественным числом). Определение находится в пакете mllib.regression package\n",
    "\n",
    "3) Rating - оценка продукта пользователем, используемая в пакете mllib.recommendation для определения рекомендаций\n",
    "\n",
    "4) семейство классов Model - все модели типа Model являются результатом работы алгоритма обучения и обычно имеют метод predict() для применения модели к новой точке или к набору RDD новых точек данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Создать плотный вектор <1.0, 2.0, 3.0>\n",
    "denseVecl = array([1.0, 2.0, 3.0]) # в MLlib можно передавать непосредственно массивы NumPy\n",
    "\n",
    "denseVec2 = Vectors.dense([1.0, 2.0, 3.0]) # .. или использовать класс Vectors\n",
    "\n",
    "# Создать разреженный вектор <1.0, О.О, 2.0, О.О>; соответствующие методы принимают только размер вектора (4) и\n",
    "# индексы с ненулевыми элементами. Исходные данные можно передать в виде словаря или как два риска - индексов и значений.\n",
    "sparseVecl = Vectors. sparse ( 4, {0: 1.0, 2: 2.0})\n",
    "sparseVec2 = Vectors.sparse(4, [0, 2], [1.0, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение признаков (mllib. feature)\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "Term Frequency - lnverse Document Frequency (частота слова-обратная частота документа), или TF-IDF - это простой алгоритм\n",
    "создания векторов признаков для текстовых документов (таких как веб-страницы). Он вычисляет для каждого слова в каждом документе две статистики: частоту слова (Term Frequency, TF), то есть сколько раз встретилось это слово в документе, и обратную частоту документа (Inverse Document Frequency, IDF), оценивающую, как (не)часто встречается слово во всей коллекции документов. Произведение этих значений, TF х IDF, показывает, насколько важным является слово для конкретного документа (то есть слово может часто использоваться в данном документе, но редко во всей коллекции в целом)\n",
    "\n",
    "В библиотеке MLlib имеются два алгоритма для вычисления оценки TF-IDF: HashingTF и IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10000, {3741: 2.0, 9145: 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "sentence = \"hello hello world\"\n",
    "words = sentence.split(\" \") #Разбить sentence на список слов\n",
    "tf = HashingTF(10000) # Создать вектор размера S = 10000\n",
    "tf.transform(words) #вычисление TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "# Прочитать множество текстовых файлов в векторы TF\n",
    "data = \"C:\\spark\\SPARK_HOME\"\n",
    "rdd = sc.wholeTextFiles(data).map(lambda (name, text): text.split())\n",
    "tfVectors = tf.transform(rdd).cache() # Преобразовать сразу весь набор документов RDD\n",
    "\n",
    "# Вычислить IDF, затем векторы TF-IDF\n",
    "idf = IDF ()\n",
    "idfModel = idf .fit (tfVectors)\n",
    "tfidfVectors = idfModel.transform(tfVectors) #TF-IDF вектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Масштабирование\n",
    "\n",
    "Класс StandardScaler из MLlib, выполняет масштабирование по средним значениям и стандартным отклонениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([-0.7071, 0.7071, 0.0]), DenseVector([0.7071, -0.7071, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "vectors = [Vectors.dense([-2.0, 5.0, 1.0]), Vectors.dense([2.0, 0.0, 1.0])]\n",
    "dataset = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "model = scaler.fit(dataset)\n",
    "result = model.transform(dataset)\n",
    "print result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([-0.3651, 0.9129, 0.1826]), DenseVector([0.8944, 0.0, 0.4472])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Normalizer\n",
    "vectors = [Vectors.dense([-2.0, 5.0, 1.0]), Vectors.dense([2.0, 0.0, 1.0])]\n",
    "dataset = sc.parallelize(vectors)\n",
    "result = Normalizer().transform(dataset) #приводит результат к единичной норме\n",
    "print result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec 1 (https://code.google.com/p/word2vec/) - это алгоритм выделения признаков для текста на основе нейронных сетей, который можно использовать для подготовки данных к обрабьтке следующими за ним алгоритмами. Фреймворк Spark включает реализацию этого алгоритма в виде класса mllib. feature. Word2Vec.\n",
    "\n",
    "Для обучения Word2Vec следует передать коллекцию документов, представленную объектами-коллекциями Iterable строк (по одному слову в строке). Так же как для алгоритма TF-IDF, рекомендуется нормализовать слова (например, привести все символы к нижнему\n",
    "регистру и удалить из текста знаки пунктуации и числа). Результатом обучения (вызовом Word2Vec .fit (rdd)) является объект Word2VecModel, метод transform() которого можно использовать для преобразования каждого слова в вектор. Обратите внимание, что размер модели в алгоритме Word2Vec равен числу слов в словаре, умноженному на размер вектора (по умолчанию 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 0.313273698092\n",
      "4: 0.207959577441\n",
      "0: 0.202663198113\n",
      "3: 0.107770711184\n",
      "9: -0.0441659018397\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "inp = sc.textFile(\"C:\\spark\\SPARK_HOME\\data\\mllib\\sample_lda_data.txt\").map(lambda row: row.split(\" \"))\n",
    "\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('1', 5)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Статистики\n",
    "\n",
    "Библиотека MLlib предлагает возможность вычисления широко используемых статистических характеристик для наборов RDD с помощью методов класса mllib.stat.Statistics, таких как: \n",
    "\n",
    "1) Statistics.colStats(rdd) - вычисляет статистические характеристики для векторов в наборе RDD: минимальное, максимальное, среднее и дисперсию для каждого столбца во множестве векторов. Может использоваться для получения широкого разнообразия статистик за один проход;\n",
    "\n",
    "2) Statistics.corr(rdd, method) - вычисляет матрицу корреляций между столбцами в наборе векторов, с использованием алгоритма Пирсона (Pearson) или Спирмена (Spearman) - определяется параметром rnethod, который может иметь значение pearson или spearrnan;\n",
    "\n",
    "3) Statistics.corr(rdd1, rdd2, method) - вычисляет корреляцию между двумя наборами RDD вещественных значений, используя алгоритм Пирсона или Спирмена - определяется параметром method, который может иметь значение pearson или spearrnan;\n",
    "\n",
    "4) Statistics.chiSqTest(rdd) - вычисляет тест независимости Пирсона для каждого признака с маркером в наборе RDD объектов LabeledPoint. Возвращает массив объектов ChiSqTestResult, хранящих р-значение, статистики теста и степень свободы для каждого признака. Значения маркера и признака должны быть качественными величинами (то есть дискретными)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LassoWithSGD, RidgeRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные классы имеют несколько параметров настройки алгоритма:\n",
    "\n",
    "1) numIterations - число итераций (по умолчанию: 100);\n",
    "\n",
    "2) stepSize - величина шага градиентного спуска (по умолчанию: 1.0);\n",
    "\n",
    "3) intercept - определяет необходимость добавления систематического признака в данные - то есть еще одного признака, значение которого всегда равно 1 (по умолчанию false);\n",
    "\n",
    "4) regParam - параметр регуляризации для регрессии Лассо и гребневой регрессии (по умолчанию 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = # (создать набор объектов LabeledPoint)\n",
    "#Экземпляр LabeledPoint состоит из маркера label (всегда являющегося вещественным числом типа Double, но в задачах \n",
    "#классификации могущего получать дискретные целые значения) и вектора признаков features\n",
    "model = LinearRegressionWithSGD.train(points, iterations=200, intercept=True)\n",
    "print \"weights: %s, intercept: %s\" % (model.weights, model.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import  LogisticRegressionWithLBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\t\n",
    "\n",
    "data – The training data, an RDD of LabeledPoint.\n",
    "\n",
    "iterations – The number of iterations. (default: 100)\n",
    "\n",
    "initialWeights – The initial weights. (default: None)\n",
    "\n",
    "regParam – The regularizer parameter. (default: 0.0)\n",
    "\n",
    "regType – The type of regularizer used for training our model. Supported values:\n",
    "“l1” for using L1 regularization\n",
    "“l2” for using L2 regularization (default)\n",
    "None for no regularization\n",
    "\n",
    "intercept – Boolean parameter which indicates the use or not of the augmented representation for training data (i.e., whether bias features are activated or not). (default: False)\n",
    "\n",
    "corrections – The number of corrections used in the LBFGS update. If a known updater is used for binary classification, it calls the ml implementation and this parameter will have no effect. (default: 10)\n",
    "\n",
    "tolerance – The convergence tolerance of iterations for L-BFGS. (default: 1e-6)\n",
    "\n",
    "validateData – Boolean parameter which indicates if the algorithm should validate data before training. (default: True)\n",
    "\n",
    "numClasses – The number of classes (i.e., outcomes) a label can take in Multinomial Logistic Regression. (default: 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = [LabeledPoint(0.0, [0.0, 1.0]), LabeledPoint(1.0, [1.0, 0.0])]\n",
    "lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(data), iterations=10)\n",
    "print(lrm.predict([1.0, 0.0]))\n",
    "print(lrm.predict([0.0, 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод опорных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1]\n",
      "1.4407421280634385\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "data = [LabeledPoint(0.0, [0.0]), LabeledPoint(1.0, [1.0]), LabeledPoint(1.0, [2.0]), LabeledPoint(1.0, [3.0])]\n",
    "\n",
    "svm = SVMWithSGD.train(sc.parallelize(data), iterations=10)\n",
    "print svm.predict([1.0])\n",
    "print svm.predict(sc.parallelize([[1.0]])).collect()\n",
    "svm.clearThreshold() #сбросить порог\n",
    "print svm.predict(array([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный байесовский классификатор\n",
    "Параметры: lambda  - сглаживание.\n",
    "\n",
    "При вызове модели передается набор RDD объектов LabeledPoints, где маркеры имеют значения от 0 до C - 1 при классификации\n",
    "в C классов.\n",
    "\n",
    "Возвращаемый объект NaiveBayesModel имеет метод predict (), возвращающий класс, которому лучше всего соответствует анализируемая точка. \n",
    "\n",
    "Также доступны два параметра обученной модели: theta, матрица вероятностей для каждого признака (размера С х D для\n",
    "С классов и D признаков), и pi, С-мерный вектор приоритетов классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for positive test example: 1\n",
      "Prediction for negative test example: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "# Выполнить логистическую регрессию методом SGD.\n",
    "model = NaiveBayes.train(trainingData)\n",
    "\n",
    "#Проверить положительный экземпляр (спам) и отрицательный (неспам).\n",
    "#Сначала применить то же преобразование HashingTF, чтобы получить векторы признаков, затем применить модель.\n",
    "\n",
    "posTest = tf.transform( \"О М G GET cheap stuff Ьу sending money to ... \".split (\" \"))\n",
    "negTest = tf.transform( \"Hi Dad, I started studying Spark the other ... \".split(\" \"))\n",
    "print \"Prediction for positive test example: %g\" %model.predict(posTest)\n",
    "print \"Prediction for negative test example: %g\" %model.predict(negTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный лес\n",
    "\n",
    "RandomForest принимает следующие параметры:\n",
    "\n",
    "1) numTrees - число создаваемых деревьев. Увеличение значения numTrees уменьшает вероятность переобучения ( overfitting) на\n",
    "обучающей выборке;\n",
    "\n",
    "2) featureSubsetStrategy - число признаков, учитываемых при разбиении каждого узла; может принимать значения: auto (библиотека\n",
    "сама выберет подходящее значение), all, sqrt, log2 и onethird; чем больше значение, тем выше стоимость алгоритма;\n",
    "\n",
    "3) seed - начальное значение для генератора случайных чисел.\n",
    "\n",
    "4) data - набор RDD объектов LabeledPoint;\n",
    "\n",
    "5) numClasses (только для классификации) - число классов;\n",
    "\n",
    "6) categoricalFeaturesinfo - ассоциативный массив, определяющий, какие признаки являются качественными и сколько категорий\n",
    "каждый из них может иметь. Например, если признак 1 имеет двоичный характер с маркерами О и 1, а признак 2 определяет\n",
    "три категории со значениями О, 1 и 2, в этом параметре нужно будет передать { 1: 2, 2: 3). Если качественных признаков\n",
    "нет, передавайте пустой ассоциативный массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 1:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "dt = [LabeledPoint(0.0, [0.0]), LabeledPoint(0.0, [1.0]),LabeledPoint(1.0, [2.0]),LabeledPoint(1.0, [3.0])]\n",
    "\n",
    "model = RandomForest.trainClassifier(sc.parallelize(dt), 2, {}, numTrees = 3, seed=42)\n",
    "#model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)\n",
    "print model.numTrees()\n",
    "print model.totalNumNodes()\n",
    "print(model)\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "# Метод К-средних\n",
    "\n",
    "Помимо значения К, реализация метода К-средних в MLlib имеет следующие параметры:\n",
    "\n",
    "1) initializationMode - метод инициализации центров кластеров, который может быть \"k-means||\" или <<random>>; \"k-means||\" (по умолчанию) обычно дает лучшие результаты, но является более дорогостоящим;\n",
    "\n",
    "2)maxIterations - максимальное число итераций (по умолчанию: 20);\n",
    "\n",
    "3) runs - число параллельных процессов, выполняющих алгоритм.\n",
    "\n",
    "4) rdd – Training points as an RDD of Vector or convertible sequence types.\n",
    "\n",
    "5) k – The desired number of leaf clusters. The actual number could be smaller if there are no divisible leaf clusters. (default: 4)\n",
    "\n",
    "6) minDivisibleClusterSize – Minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. (default: 1)\n",
    "\n",
    "7) seed – Random seed value for cluster initialization. (default: -1888008604 from classOf[BisectingKMeans].getName.##)\n",
    "\n",
    "Реализация К-средних в MLlib поддерживает параллельное выполнение вычислений с разных начальных позиций, с выбором\n",
    "лучшего результата, что позволяет получить лучшую полную модель (так как вычисления методом К-средних могут\n",
    "быть остановлены в локальном минимуме)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([366.75]), array([-383.]), array([29.95])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array \n",
    "data = array([-383.1,-382.9, 28.7,31.2, 366.2,367.3]).reshape(6,1)\n",
    "model = KMeans.train(sc.parallelize(data), 3, maxIterations=5)\n",
    "model.clusterCenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод чередующихся наименьших квадратов\n",
    "\n",
    "MLlib включает реализацию метода чередующихся наименьших квадратов (Alternating Least Squares, ALS), популярного алгоритма\n",
    "коллаборативной фильтрации, который прекрасно масштабируется для выполнения на кластерах. Он находится в классе mllib.recommendation.ALS.\n",
    "\n",
    "Алгоритм ALS определяет вектор признаков для каждого пользователя и товара - такой, чтобы скалярное произведение вектора пользователя и товара было максимально близким к их оценке. \n",
    "\n",
    "Имеет следующие параметры:\n",
    "\n",
    "1) rank - размер векторов признаков; чем больше значение rank, тем точнее модель, но выше стоимость вычислений (по умолчанию:\n",
    "10);\n",
    "\n",
    "2) iterations - число итераций (по умолчанию: 10);\n",
    "\n",
    "3) lambda - регулирующий параметр (по умолчанию: 0.01);\n",
    "\n",
    "4) alpha - константа, используемая для вычисления уровня надежности неявных оценок (по умолчанию: 1.0);\n",
    "\n",
    "5) numUserBlocks, numProductВlocks - число блоков для разделения пользователей и товаров при параллельных вычислениях; можно\n",
    "передать значение -1 (по умолчанию), чтобы библиотека MLlib могла автоматически подобрать значение для этого параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "# Load and parse the data\n",
    "path = \"C:\\spark\\SPARK_HOME\\data\\mllib\\als\\test.data\"\n",
    "data = sc.textFile(path)\n",
    "ratings = data.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/myCollaborativeFilter\")\n",
    "#sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "# Compute the top 4 principal components.\n",
    "# Principal components are stored in a local dense matrix.\n",
    "pc = mat.computePrincipalComponents(4) \n",
    "\n",
    "# Project the rows to the linear space spanned by the top 4 principal components.\n",
    "projected = mat.multiply(pc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(5, {1: 1.0, 3: 7.0}),\n",
       " DenseVector([2.0, 0.0, 3.0, 4.0, 5.0]),\n",
       " DenseVector([4.0, 0.0, 0.0, 6.0, 7.0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.rows.collect() #данные в начальной системе координат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.48591721e-01,  1.33019857e-01, -1.25231564e-01,  2.16507567e-01,\n",
       "       -8.47651293e-01, -2.84238082e-01, -5.62115590e-02,  7.63626477e-01,\n",
       "       -5.65295877e-01, -1.15603405e-01,  8.34454526e-02,  4.42397926e-02,\n",
       "       -5.78071229e-01, -7.95540506e-01, -1.55011789e-01,  8.36410201e-01,\n",
       "        1.72243378e-01,  2.55415489e-01,  4.85812143e-05, -4.53335549e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.values #Вектора на которые проецируются данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.6486, -4.0133, -5.5245, 0.1726]),\n",
       " DenseVector([-4.6451, -1.1168, -5.5245, 0.1726]),\n",
       " DenseVector([-6.4289, -5.338, -5.5245, 0.1726])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected.rows.collect() #спроецированные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сингулярное разложение матрицы\n",
    "\n",
    "U - ортонормальная матрица, столбцы которой называют левыми сингулярными векторами;\n",
    "\n",
    "s - диагональная матрица с неотрицательными диагональными элементами, расположенными в порядке убывания, которые называют сингулярными значениями;\n",
    "\n",
    "V - ортонормальная матрица, столбцы которой называют правыми сингулярными векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top 5 singular values and corresponding singular vectors.\n",
    "svd = mat.computeSVD(5, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
