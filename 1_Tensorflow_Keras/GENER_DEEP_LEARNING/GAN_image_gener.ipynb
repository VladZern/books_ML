{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генеративно-состязательные сети (Generative Adversarial Networks, GAN)\n",
    "\n",
    "Генеративно-состязательная сеть состоит из двух сетей — выполняющей подделку и оценивающей эту подделку, постепенно обучающих друг друга:\n",
    "\n",
    "1) сеть-генератор — получает на входе случайный вектор (случайную точку в скрытом пространстве) и декодирует его в искусственное изображение;\n",
    "\n",
    "2) сеть-дискриминатор (или противник) — получает изображение (настоящее или поддельное) и определяет, взято ли это изображение из обучающего набора или сгенерировано сетью-генератором.\n",
    "\n",
    "Сеть-генератор обучается обманывать сеть-дискриминатор и, соответственно, учится создавать все более реалистичные изображения: поддельные изображения, неотличимые от настоящих в той мере, на какую способна сеть-дискриминатор. Сеть-дискриминатор, в свою очередь, постоянно адаптируется к увеличивающейся способности сети-генератора и устанавливает все более высокую планку реализма для генерируемых изображений. По окончании обучения генератор способен превратить любую точку из своего входного пространства в правдоподобное изображение. В отличие от вариационных автокодировщиков, это скрытое пространство дает меньше гарантий наличия в нем значимой структуры; в частности, оно не является непрерывным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная реализация — глубокая сверточная генеративно-состязательная сеть (Deep Convolutional GAN, DCGAN), в которой генератор и дискриминатор являются глубокими сверточными сетями. В ней, например, используется слой Conv2DTranspose для увеличения разрешения изображения в генераторе.\n",
    "Мы будем обучать GAN на изображениях из набора CIFAR10, содержащего 50 000 изображений 32 × 32 в формате RGB, которые делятся на 10 классов (по 5000 изображений в каждом классе). Для простоты мы используем только изображения, принадлежащие классу «лягушка».\n",
    "В общих чертах GAN выглядит примерно так:\n",
    "    \n",
    "1) Сеть generator отображает векторы с формой (размерность_скрытого_пространства,) в изображения с формой (32, 32, 3).\n",
    "\n",
    "2) Сеть discriminator отображает изображения с формой (32, 32, 3) в оценку вероятности того, что изображение является настоящим.\n",
    "\n",
    "3) Сеть gan объединяет генератор и дискриминатор gan(x) = discriminator(generator(x)). То есть сеть gan отображает скрытое пространство векторов в оценку реализма этих скрытых векторов, декодированных генератором.\n",
    "\n",
    "4) Мы обучим дискриминатор на примерах реальных и искусственных изображений, отмеченных метками «настоящее»/«поддельное», как самую обычную модель классификации изображений.\n",
    "5) Для обучения генератора мы используем градиенты весов генератора в отношении потерь модели gan. То есть на каждом шаге мы будем смещать веса генератора в направлении увеличения вероятности классификации дискриминатором изображений, декодированных генератором как «настоящие». Иными словами, мы будем обучать генератор обманывать дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Набор хитростей\n",
    "\n",
    "1) В качестве последней функции активации в генераторе мы используем tanh вместо sigmoid, которую часто можно встретить в моделях других типов.\n",
    "\n",
    "2) Мы будем выбирать точки из скрытого пространства, используя нормальное распределение (распределение Гаусса), а не равномерное.\n",
    "\n",
    "3) Стохастичность повышает устойчивость. Поскольку целью обучения является динамическое равновесие, генеративно-состязательные сети легко могут застревать на разных препятствиях. Введение случайной составляющей в процесс обучения помогает предотвратить это. Мы вводим случайный компонент двумя способами: используя прореживание в дискриминаторе и добавляя случайный шум в метки для дискриминатора.\n",
    "\n",
    "4) Разреженные градиенты могут препятствовать обучению GAN. В глубоком обучении разреженность часто является желательным свойством, но не в случае с GAN. Разреженность градиента могут вызывать: операции выбора максимального значения по соседним элементам (max pooling) и активации ReLU. Вместо выбора максимального значения для уменьшения разрешения мы рекомендуем использовать чередующиеся свертки, а вместо функции активации ReLU — слой LeakyReLU. Он напоминает ReLU, но ослабляет ограничение разреженности, допуская небольшие отрицательные значения активации.\n",
    "\n",
    "5) В сгенерированных изображениях часто наблюдаются артефакты типа «шахматная доска», обусловленные неравномерным охватом пространства пикселов в генераторе (рис. 8.17). Для их устранения мы будем выбирать размер ядра, кратный размеру шага, при каждом использовании разреженных слоев Conv2DTranpose или Conv2D в генераторе и дискриминаторе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "generator_input = tensorflow.keras.Input(shape=(latent_dim,))\n",
    "\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input) # Преобразование входа в карту признаков 16 × 16 со 128 каналами\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x) \n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x) # Увеличение разрешения до 32 × 32\n",
    "x = layers.LeakyReLU()(x) \n",
    "\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x) \n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x) # Производит карту признаков 32 × 32 с 1 каналом (форма изображений в наборе CIFAR10)\n",
    "generator = tensorflow.keras.models.Model(generator_input, x) # Создание модели генератора, которая отображает вход с формой (размерность_скрытого_пространства,) в изображение с формой (32, 32, 3)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дискриминатор\n",
    "\n",
    "Принимает на входе изображение-кандидат (реальное или искусственное) и относит его к одному из двух классов: «подделка» или «настоящее, имеющееся в обучающем наборе»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x) #Уровень прореживания: важная хитрость!\n",
    "x = layers.Dense(1, activation='sigmoid')(x) #Уровень классификации\n",
    "\n",
    "discriminator = tensorflow.keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "discriminator_optimizer = tensorflow.keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8) # Для стабилизации используется затухание скорости обучения\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Состязательная сеть\n",
    "\n",
    "Перейдем к состязательной сети, объединяющей генератор и дискриминатор. В процессе обучения эта модель будет смещать веса генератора в направлении увеличения способности обмана дискриминатора. Эта модель преобразует точки скрытого пространства в классифицирующее решение — «подделка» или «настоящее» — и предназначена для обучения с метками, которые всегда говорят: «это настоящие изображения». То есть обучение gan будет смещать веса в модели generator так, чтобы увеличить вероятность получить от дискриминатора ответ «настоящее», когда тот будет просматривать поддельное изображение. Важно также отметить, что дискриминатор нужно «заморозить» на время обучения (отключить его обучение): его веса не должны обновляться при обучении gan. В противном случае все сведется к тому, что вы обучите дискриминатор всегда отвечать «настоящее»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False #заморозка дискриминатора\n",
    "\n",
    "gan_input = tensorflow.keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "\n",
    "gan = tensorflow.keras.models.Model(gan_input, gan_output)\n",
    "gan_optimizer = tensorflow.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как обучить сеть DCGAN\n",
    "\n",
    "Теперь можно приступать к обучению. Ниже схематически описывается общий цикл обучения. В каждой эпохе нужно выполнить следующие действия:\n",
    "\n",
    "1.Извлечь случайные точки из скрытого пространства (случайный шум).\n",
    "\n",
    "2.Создать изображения с помощью генератора, использовав случайный шум.\n",
    "\n",
    "3.Смешать сгенерированные изображения с настоящими.\n",
    "\n",
    "4.Обучить дискриминатор на этом смешанном наборе изображений, добавив соответствующие цели: «настоящее» (для настоящих изображений) или «подделка» (для сгенерированных изображений).\n",
    "\n",
    "5.Выбрать новые случайные точки из скрытого пространства.\n",
    "\n",
    "6.Обучить gan, использовав эти случайные векторы, с целями, которые всегда говорят: «это настоящие изображения». Это приведет к смещению весов генератора (и только генератора, потому что внутри gan дискриминатор «замораживается») в направлении, увеличивающем вероятность получить от дискриминатора ответ «настоящее» для сгенерированных изображений: это научит генератор обманывать дискриминатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 1435s 8us/step\n",
      "discriminator loss: 0.6877128\n",
      "adversarial loss: 0.69653255\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "(x_train, y_train), (_, _) = tensorflow.keras.datasets.cifar10.load_data() #Загрузка данных CIFAR10\n",
    "\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = 'C:/Users/Vladuk/1_books_ML/1_Tensorflow_Keras/GENER_DEEP_LEARNING/pic' #каталог для сохранения сгенерированных изображений \n",
    "start = 0\n",
    "\n",
    "for step in range(iterations): \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) #Выбор случайных точек из скрытого пространства\n",
    "    \n",
    "    generated_images = generator.predict(random_latent_vectors) #создание поддельного изображения\n",
    "    \n",
    "    stop = start + batch_size \n",
    "    real_images = x_train[start: stop] \n",
    "    combined_images = np.concatenate([generated_images, real_images])  #объединение поддельного изображения с настоящими\n",
    "    \n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))]) #cборка меток отличающих настоящее изображение от поддельного\n",
    "    \n",
    "    labels += 0.05 * np.random.random(labels.shape) #добавление случайного шума в метки\n",
    "    \n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels) #обучение дискриминатора\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) #выбор случайных точек из скрытого пространства\n",
    "    \n",
    "    misleading_targets = np.zeros((batch_size, 1)) #сборка меток которые всегда говорят, что это настоящие изображения\n",
    "    \n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets) #обучение генератора через gan\n",
    "    start += batch_size \n",
    "    if start > len(x_train) - batch_size: \n",
    "        start = 0 \n",
    "    if step % 100 == 0: #сохранение изображения через каждые 100 шагов\n",
    "        gan.save_weights('gan.h5') # сохранение весов модели\n",
    "        \n",
    "        print('discriminator loss:', d_loss) #вывод метрик\n",
    "        print('adversarial loss:', a_loss) \n",
    "        \n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)  #сохранение одного сгенерированного изображения\n",
    "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png')) \n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False) #сохранение настойщего изображения для сравнения\n",
    "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подведение итогов\n",
    "\n",
    "1) Генеративно-состязательная сеть состоит из двух сетей: генератора и дискриминатора. Дискриминатор обучается отличать изображения, созданные генератором, от настоящих, имеющихся в обучающем наборе, а генератор обучается обманывать дискриминатор. Примечательно, что генератор вообще не видит изображений из обучающего набора; вся информация, которую он имеет, поступает из дискриминатора.\n",
    "\n",
    "2) Генеративно-состязательные сети сложны в обучении, потому что обучение GAN — это динамический процесс, отличный от обычного процесса градиентного спуска по фиксированному ландшафту потерь. Для правильного обучения GAN приходится использовать ряд эвристических трюков, а также уделять большое внимание настройкам.\n",
    "\n",
    "3) Генеративно-состязательные сети потенциально способны производить очень реалистичные изображения. Однако в отличие от вариационных автокодировщиков получаемое ими скрытое пространство не имеет четко выраженной непрерывной структуры, и поэтому они могут не подходить для некоторых практических применений, таких как редактирование изображений с использованием концептуальных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
